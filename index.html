<!doctype html>
<html lang="en">
  <head>
    <title>Yunzhe Jonas Li: Forge The Future</title>

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <link href="./assets/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

  </head>
  <body>
    <div style="height: 4em"></div>
    <section
      style="
        display: flex;
        align-items: start;
        gap: 2em 1.5em;
        flex-wrap: wrap-reverse;
      "
    >
      <div style="flex-grow: 1; flex-basis: 20em">
        <h1>Yunzhe "Jonas" Li</h1>
        <!-- <p>where I am affiliated with the <a href="https://msc.berkeley.edu/">Mechanical Systems Control Lab</a> to work on humanoid robot manipulation</p> -->

      <p>
        I am currently a Machine Learning Engineer at 
        <a href="https://www.weride.ai/">WeRide</a>, working on the 
        <a href="https://youtu.be/bXMyts8NMdk?si=TQwBEuW8hEEkc7GT">WePilot AiDrive</a> system. My focus is on exploring Vision Language Models to overcome the limitations 
        in understanding driving intent and human interaction.
      </p>

      <p>
        Previously, I interned as a software engineer at 
        <a href="https://www.momenta.cn/en/">Momenta</a>, where I contributed to the 
        autopilot reversing feature for GM Cadillac LYRIQ,
        reducing false positives by 15% on 800+ real parking test cases across 30+ garages.
      </p>

      <p>
        I earned my master’s degree in the Department of EECS at 
        <a href="https://eecs.berkeley.edu/">UC Berkeley</a> in 2025 -- 
        <i><b style="color:#003262; background-color: #FDB515;">Fiat Lux and Go Bears</b></i>, and my B.E. in Computer Science from 
        <a href="https://en.shu.edu.cn/">Shanghai University</a> in 2024.
      </p>

      <p>
        My passion for the intersection of computer vision, AI, and robotics began in high school, 
        where I led a team in the 
        <a href="https://www.firstinspires.org/robotics/ftc">FIRST Tech Challenge</a>. 
        From 2017 to 2020, we earned 2 FIRST World Championship qualifications, 1 Inspire Award, 
        and 4 Connect Awards.
      </p>

      <p>
        In <a href="https://www.robomaster.com/en-US">RoboMaster</a> competition from 2020 to 2024, 
        I co-led the computer vision subteam and later served as overall lead of a 40-member squad, where we built 
        eight types of robots from scratch and won 3rd place in the Shanghai Regional Competition. 
        I am graciously to be an alumnus of both <a href="https://www.robomaster.com/en-US">RoboMaster</a> and <a href="https://www.firstinspires.org/">FIRST</a>, 
        and affiliated with <a href="https://www.dji.com/">DJI</a> as an event technical executive.
      </p>


        <p>
          <a href="mailto:liyunzhe.jonas@berkeley.edu">Email</a>
          &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/yunzhe-l-991638151/">LinkedIn</a>
          &nbsp;/&nbsp;
          <a href="https://www.youtube.com/@yunzheli5359">YouTube</a>
          &nbsp;/&nbsp;
          <a href="https://github.com/LIYunzhe1408">Github</a>
          &nbsp;/&nbsp;
          <a href="https://drive.google.com/file/d/17jS-61mbPj8e4mEsLsay-TkfmC1k8RP8/view?usp=sharing">Resume</a>
        </p>
      </div>
      <div style="display: block">
        <img
          src="./assets/SF.jpg"
          alt="A photo of the author"
          style="
            width: 20em;
            height: auto;
            aspect-ratio: 1;
            object-fit: cover;
            border-radius: 10%;
          "
        />
      </div>
    </section>

    <section id="filters">
      <h2>Projects</h2>
      <p> "What motivates the development of AI must explicitly center on human benefit." -- Fei-Fei Li</p>
      <button data-category="featured" onclick="filterPapers('featured')">Featured</button>
      <button data-category="all" onclick="filterPapers('all')">All</button>
      <button data-category="CVR" onclick="filterPapers('CVR')">Robotics & Computer Vision</button>
      <button data-category="LLM" onclick="filterPapers('LLM')">LLM & ML Application</button>
    </section>
    
    <section>
      <!-- Paper list will be dynamically generated here -->
      <ul id="paper-list" style="margin-top: 1em" data-show-recent="false"></ul>
      <!-- [ <a href="#" onclick="toggleRecentPapers(event)">Show older projects</a> ] -->

      <!-- Generate paper list. -->
      <script>
        const papers = [
          {
            previewSrc: "./assets/Optibots.gif",
            title:
              "OptiBots: Modular Robot Design Using Genetic Algorithms",
            authors: [
              "Jae Won Kim", "Yunzhe \"Jonas\" Li", "Calix Tang", "Alexandra Zhang Jiang", "Shankar Kailas",
            ],
            conference: "Capstone Project @ UC Berkeley, 2025",
            
            category: "CVR",
            featured: false,
            links: [
              { text: "Report", url: "https://drive.google.com/file/d/10FZ_gAAY2tnsnTH_F4qJB8uyd5-BorPb/view?usp=drive_link" },
              { text: "Website", url: "https://yunzhe-li.top/OptiBots/" },
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/OptiBots" },
            ],
            summary:
              "Advised by Prof. Masayoshi Tomizuka, Dr. Wei Zhan, Yuxin Chen, and Chenran Li, OptiBots automates modular robot design and control using genetic algorithms and reinforcement learning, enabling fast, low-cost prototyping in simulation.",
            highlighted: false,
            recent: true,
            new: true,
        },  
          {
            previewSrc: "./assets/Video FlexToK.mp4",
            title:
              "FlexTok for Video Reconstruction",
            authors: [
              "Zikai Liu*", "Yunzhe \"Jonas\" Li*", "Advait Gosai*",
            ],
            conference: "CS280 @ UC Berkeley, 2025",
            
            category: "CVR",
            featured: true,
            links: [
              { text: "Report", url: "https://drive.google.com/file/d/18d3QyqxoFDTTRwvBRvx2NwESfzbkK1qc/view?usp=drive_link" },
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/video-tokenizer" },
            ],
            summary:
              "Instructed by Prof. Jitendra Malik & Prof. Angjoo Kanazawa, this project extends a 1D image tokenizer to the video domain and evaluate its ability to preserve visual and temporal structure on exocentric, egocentric, and synthetic datasets across different token lengths.",
            highlighted: false,
            recent: true,
            new: false,
        },  
        {
            previewSrc: "./assets/MNIST generate.gif",
            title:
              "Flow Matching For MNIST",
            authors: [
              "Yunzhe \"Jonas\" Li",
            ],
            conference: "CS 280 @ UC Berkeley, 2025",
            
            category: "CVR",
            featured: false,
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Flow-Matching" },
              { text: "Website", url: "https://yunzhe-li.top/Flow-Matching/" },
            ],
            summary:
              "Instructed by Prof. Jitendra Malik & Prof. Angjoo Kanazawa, this project explores Flow Matching as a generative modeling technique by training a UNet-based denoiser on the MNIST dataset. The model progressively denoises images using flow matching instead of traditional diffusion models.",
            highlighted: false,
            recent: true,
            new: false,
          },  
        {
            previewSrc: "./assets/Abstract-concise.mp4",
            title:
              "Abstract: Your LeetCode Learning Assistant",
            authors: [
              "Yunzhe \"Jonas\" Li",
            ],
            conference: "Open-Source Contribution, 2025",
            category: "LLM",
            featured: false,
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Abstract-LeetCode" },
              { text: "Video", url: "https://youtu.be/IpndzIX_nuw?si=3FaO41SYUJSazVCq" },
            ],
            summary:
              "ABSTRACT is an intelligent study and review tool designed specifically for mastering LeetCode questions. It helps users organize, summarize, and retain key problem-solving patterns efficiently. With structured note-taking, spaced repetition, and AI-powered insights, ABSTRACT makes coding interview preparation faster and more effective.",
            highlighted: false,
            recent: true,
            new: false,
          },  
        {
            previewSrc: "./assets/MealMate.mp4",
            title:
              "MealMate: From Craving to Cart",
            authors: [
              "Yunzhe \"Jonas\" Li",
              "Jasper Liu",
              "Mats Wiig Martinussen",
              "Emil Klovning",
              "Jason Ji",
              "Qinghe Zheng",
            ],
            conference: "LLM Agents MOOC Hackathon, Berkeley RDI, 2024",
            category: "LLM",
            featured: false,
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/MealMate" },
              { text: "Video", url: "https://youtu.be/bAT-jZhDtCw?si=r7iHjxn-TGihWYJS" },
            ],
            summary:
              "MealMate is an intelligent meal planning assistant that simplifies food shopping by seamlessly integrating recipe generation, ingredient availability checks, and personalized substitutions. Powered by advanced multi-agent collaboration and large language models (LLMs), MealMate delivers tailored shopping lists based on user preferences and real-time store inventory.",
            highlighted: false,
            recent: true,
            new: false,
          },  
          {
            previewSrc: "./assets/RuneTrack.mp4",
            title:
              "RuneTrack: Real-Time Planar Non-Uniform Rotation Targeting",
            authors: [
              "Yunzhe \"Jonas\" Li",
              "Xinyu He",
              "Jun Chen",
              "Minfeng Tang",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2023",
            category: "CVR",
            featured: true,
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "The RuneTrack enhances real-time Energy Rune targeting in RoboMaster by replacing OpenCV-based detection with a YOLOv7-powered solution. A fine-tuned Rune detector, combined with a Least Squares-based tracker, retains 95% accuracy at 60 FPS on NVIDIA Jetson NX. The system computes roll, pitch, and yaw angles based on geometric information, ensuring precise gimbal control. The pipeline improves targeting stability and achieves 90% hit rate in dynamic competition scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/EngineerRobot.mp4",
            title:
              "OmniMiner: Adaptive Engineer Robot for High-Speed Object Collection and Redemption",
            authors: [
              "Chen Chen",
              "Daoming He",
              "Yunzhe \"Jonas\" Li",
              "Jingyi Li",
              "Kaicheng Li",
              "Hanqing Huang",
              "Qiming Pu"
            ],
            conference: "RoboMaster, 2023",
            category: "CVR",
            featured: true,
            links: [
              { text: "Video", url: "https://youtu.be/qeRvdmcNFAc?si=4-hF27Y6jXwbmtPL" },
            ],
            summary:
              "The Engineer Robot is a 5-axis robotic system on a mecanum-wheel chassis for efficient cubic object collection and redemption in RoboMaster. Using 11 motors to control XYZ, pitch, and roll, an air pump and suction cup system handles 20cm × 20cm × 20cm cubes. Automated sequences enable 0.8s intake time. A user-friendly UI ensures smooth operation, averaging 15s for a random position and orientation placement, outperforming 95% in intake speed and 70% in placement success rate.",
            highlighted: false,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/ArmorTrack.mp4",
            title:
              "ArmorTrack: A Full-Stack Vision Pipeline for Real-Time Armor Detection and Targeting",
            authors: [
              "Minfeng Tang",
              "Xinyu He",
              "Jun Chen",
              "Yunzhe \"Jonas\" Li",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2022",
            category: "CVR",
            featured: true,
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "This ArmorTrack is a real-time, adaptive vision pipeline for all RoboMaster vision systems, integrating camera calibration, image preprocessing, YOLOv7 detection, Kalman filter tracking, and fire control. Optimized for NVIDIA Jetson NX (60+ FPS), it features enemy motion mode detection, SolvePnP aiming correction, and robust exception handling for dynamic scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/HCE.mp4",
            title:
              "Hierarchical Concept-based Visual Explainer For Deep Learning Decisions",
            authors: [
              "Yunzhe \"Jonas\" Li",
              "Ziyi Yu",
              "Yue Liu",
            ],
            conference: "State Key Program of National Nature Science Foundation of China (No.61936001), Bachelor's Thesis, 2024",
            category: "LLM",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Deep-Learning-Explainer/tree/v1.0-dataset" },
              { text: "Video", url:"https://youtu.be/9Tyr53aP-r8?si=XDSroY1QU7EiMskY"}
            ],
            summary:
              "HCE improves DNN explainability with a hierarchical concept-based method, extracting multilevel concepts via segmentation and clustering. It builds concept trees with an autoencoder and quantifies importance using Concept Tree Shapley values. A web explainer showcases HCE’s effectiveness, demonstrating 35% improved consistency in explanations across multiple image classification models.",
            highlighted: false,
            recent: true,
            new: false,
            featured: true
          },  
          {
            previewSrc: "./assets/NASICON.mp4",
            title:
              "Mining Property Relations of NASICON Solid Electrolyte",
            authors: [
              "Yunzhe \"Jonas\" Li",
              "Licheng Zhen",
              "Jiayao Zhang",
              "Xianyuan Ge",
              "Ziyi Yu",
              "Dahui Liu",
              "Yue Liu",
            ],
            conference: "General Program of National Natural Science Foundation of China (No.52073169), 2021",
            category: "LLM",
            links: [
            ],
            summary:
              "This study enhances materials science text mining by improving Named Entity Recognition (NER) and relation extraction. A BERT-based pipeline extracts 106,896 entities and 260,475 entity-relation triples from 1,808 papers, boosting NER precision by 5%. A web platform enables material scientists to convert literature into property graphs for knowledge discovery.",
            highlighted: false,
            recent: true,
            new: false,
            
            featured: false,
          }, 
        ];
        function generatePaperList(filteredCategory = 'featured') {
          const paperList = document.getElementById("paper-list");
          paperList.innerHTML = ""; // clear existing list

          let filteredPapers = [];

          if (filteredCategory === 'featured') {
            // Show only the projects marked as featured
            filteredPapers = papers.filter(paper => paper.featured);
          } else {
            filteredPapers = papers.filter(paper => {
              return filteredCategory === 'all' || paper.category === filteredCategory;
            });
          }

          filteredPapers.forEach((paper) => {
            const li = document.createElement("li");
            if (paper.highlighted) {
              li.classList.add("paper-highlighted");
            }
            if (paper.recent) {
              li.classList.add("paper-recent");
            }
            li.classList.add("paper");
            li.innerHTML = `
              ${
                paper.previewSrc.endsWith(".mp4")
                  ? `<video class="paper-visual" src="${paper.previewSrc}" loop muted autoplay playsinline></video>`
                  : `<img class="paper-visual" src="${paper.previewSrc}" alt="${paper.title}">`
              }
              <div class="paper-textual">
                <h3>
                  ${paper.title}
                  ${paper.new ? '<img src="./assets/new_animated.gif" alt="New icon"/>' : ""}
                </h3>
                <div style="height: 0.1em"></div>
                ${paper.authors
                  .map((author) => {
                    const names = author.split(" ");
                    const firstName = names.slice(0, -1).join("&nbsp;");
                    const lastName = names[names.length - 1];
                    const formattedName = `${firstName}&nbsp;${lastName}`;
                    return author.startsWith("Yunzhe \"Jonas\" Li")
                      ? `<strong>${formattedName}</strong>`
                      : formattedName;
                  })
                  .join(", ")}.
                <span style="font-weight: 450">${paper.conference}.</span>
                <div style="height: 0.375em"></div>
                ${paper.links
                  .map((link) => `<a href="${link.url}">${link.text}</a>`)
                  .join(" / ")}
                <div style="height: 0.375em"></div>
                <em style="opacity: 0.625">${paper.summary}</em>
              </div>
            `;
            paperList.appendChild(li);
          });
        }

        // Filter function for filter buttons
        function filterPapers(category) {
          generatePaperList(category);

          // Remove active class from all buttons
          const buttons = document.querySelectorAll("#filters button");
          buttons.forEach((btn) => btn.classList.remove("active"));

          // Add active class to the matching button by data-category
          const activeBtn = Array.from(buttons).find(
            (btn) => btn.dataset.category === category
          );
          if (activeBtn) {
            activeBtn.classList.add("active");
          }
        }

        document.addEventListener("DOMContentLoaded", () => {
          filterPapers("featured"); // highlights "All" button and renders all papers
        });



        function toggleRecentPapers(event) {
          event.preventDefault();
          const paperList = document.getElementById("paper-list");
          const showRecent = paperList.dataset.showRecent === "true";
          paperList.dataset.showRecent = (!showRecent).toString();
          event.target.textContent = showRecent
            ? "Show older papers"
            : "Hide older papers";
        }

      </script>
    </section>

        <section>
      <h2>My Teams and Robots</h2>
      <h3>Team Photos</h3>
      <div class="carousel-container">
        <div class="carousel-track" id="carousel-team"></div>
      </div>

      <h3>Robot Photos</h3>
      <div class="carousel-container">
        <div class="carousel-track" id="carousel-robot"></div>
      </div>

      <!-- Modal -->
      <div id="imageModal" class="image-modal">
        <div class="modal-content">
          <span class="close-button" onclick="hideModal()">×</span>
          <img id="modalImage" src="" alt="Large Robot Image" />
          <div id="modalCaption" class="modal-caption"></div>
        </div>
      </div>


      <script>
        const teamTrack = document.getElementById('carousel-team');
        const robotTrack = document.getElementById('carousel-robot');

        const robotImages = [
          {
            src: "FTC2018.JPG",
            caption: "FTC 2018 Qualifier <br/> @ SAS, Shanghai, China",
            description: `
              <strong>FTC 2018 - SAS Qualifier</strong>: Inspire Award achieved.<br/>
              Coach: Joe Xu<br/>
              Members: Yunzhe Jonas Li, Zheng Yuan, Jie Liu, Wen Wang, Haozhe Wang,<br/>
              Jinqian Zhu, Yunyi Ye, Kewei Yin, Haojie Zhang, Sichen Wang, Yixin Wang, Chuheng Chen`,
            type: "team"
          },
          {
            src: "FTCWorld.jpg",
            caption: "FRIST 2018 World Championships @ Detroit, MI, USA",
            description: "Representing China.",
            type: "team"
          },
          {
            src: "FTCChina.JPG",
            caption: "FIRST 2018 China Finals <br/> @ Suzhou, Jiangsu, China",
            description: "I joined with Zheng Yuan, Jasper Liu, Ellen Wang to showcase at the stage of FIRST CHINA. At this tournament, we F.G. got the third Connect Award in the season.",
            type: "team"
          },
          {
            src: "FTC2019.JPG",
            caption: "FTC 2019 Qualifier <br/> @ Shanghai, China",
            description: "Mechanum drivetrain robot.",
            type: "team"
          },
          {
            src: "FTC2020.jpg",
            caption: "FTC 2020 Off Season <br/> @ Shanghai, China",
            description: "Designed for stacking challenge.",
            type: "team"
          },
          {
            src: "SRM2022.png",
            caption: "RoboMaster 2022 <br/> @ Changzhou, Jiangsu, China",
            description: "YOLO-powered targeting system.",
            type: "team"
          },
          {
            src: "SRM2023.jpg",
            caption: "RoboMaster 2023 <br/> @ Shanghai, China",
            description: "3rd place at Shanghai Regional.",
            type: "team"
          },
          {
            src: "SRM2023-2.jpg",
            caption: "RoboMaster 2023 <br/> @ Shanghai University, China",
            description: "Advanced gimbal control.",
            type: "team"
          },
          {
            src: "RM2024.jpg",
            caption: "RM 2024",
            description: "National Finals, high-accuracy firing.",
            type: "team"
          },
          {
            src: "R-2017.JPG",
            caption: "Soccer Robot 2017",
            description: " My very first robot.",
            type: "robot"
          },
          {
            src: "R-FTC2018.jpg",
            caption: "FTC 2018",
            description: " Build phase moments.",
            type: "robot"
          },
          {
            src: "R-FTC2019.jpg",
            caption: "FTC 2019",
            description: " Robot close-up.",
            type: "robot"
          },
          {
            src: "R-FTC2020.jpg",
            caption: "FTC 2020",
            description: " Arm + gripper test.",
            type: "robot"
          },
          {
            src: "R-FTC2021.jpg",
            caption: "FTC 2021",
            description: " Drive system rebuild.",
            type: "robot"
          },
          {
            src: "SRM-Engineer.jpg",
            caption: "RoboMaster Engineer Robot",
            description: " 5-axis robot for cube intake.",
            type: "robot"
          },
          {
            src: "SRM-HERO.jpg",
            caption: "RoboMaster Hero Robot",
            description: " Mobile offensive unit.",
            type: "robot"
          },
          {
            src: "SRM-Sentry.jpg",
            caption: "RoboMaster Sentry Robot",
            description: " Auto-tracking defender bot.",
            type: "robot"
          },
          {
            src: "SRM-Standard.jpg",
            caption: "RoboMaster Standard Robot",
            description: " Core RM battle robot.",
            type: "robot"
          },
          {
            src: "ALL.jpg",
            caption: "All Robots Together",
            description: " Team legacy.",
            type: "robot"
          }
        ];


        robotImages.forEach(img => {
          const item = document.createElement('div');
          item.className = 'carousel-item';
          item.onclick = () => showModal(`./assets/robots/${img.src}`, img.description);
          item.innerHTML = `
            <img src="./assets/robots/${img.src}" alt="${img.caption}" />
            <div class="caption">${img.caption}</div>
          `;

          if (img.type === "team") {
            teamTrack.appendChild(item);
          } else {
            robotTrack.appendChild(item);
          }
        });
        

        function showModal(src, caption) {
          const modal = document.getElementById('imageModal');
          const modalImg = document.getElementById('modalImage');
          const modalCaption = document.getElementById('modalCaption');
          modalImg.src = src;
          modalCaption.textContent = caption;
          modalCaption.innerHTML = caption;
          modal.style.display = 'flex';
        }

        // 隐藏模态框函数
        function hideModal() {
          document.getElementById('imageModal').style.display = 'none';
        }
        function cloneCarouselItems(containerId) {
          const track = document.getElementById(containerId);
          const items = Array.from(track.children);
          items.forEach(item => {
            const clone = item.cloneNode(true);
            clone.classList.add('clone');
            track.appendChild(clone);
          });
        }

        // 在内容加载完后调用：
        cloneCarouselItems('carousel-team');
        cloneCarouselItems('carousel-robot');

        // 新增点击背景区域关闭模态框逻辑
        document.getElementById('imageModal').addEventListener('click', function (e) {
          const modalContent = document.querySelector('.modal-content');
          if (!modalContent.contains(e.target)) {
            hideModal();
          }
        });
      </script>
    </section>


    <section>
      <h2>Service</h2>
      <h3>Community Services</h3>
      <p>[2024/04/02] Head Referee at DJI RoboMaster 2024 Shanghai Regional Competition</p>
      <p>[2024/01/27] Lead Robot Inspector at 2023-2024 FIRST Tech Challenge Shanghai Qualifier</p>
      <p>[2021/05/14] Referee at 2020-2021 FIRST Tech Challenge Hangzhou Qualifier</p>
      <p>[2020/09/20] Robot Inspector at 2019-2020 FIRST Tech Challenge Shanghai Offseason Event</p>

      <h3>Media Coverage</h3>
      <p>[2024/04/02] Invited talk at DJI RoboMaster 2024 <a href="https://www.media.xinhuamm.net/html/rft/vod.html?programId=df2a93cb4ef04a20adebe284fe17055a&vodId=cab0a811860000014e301c601fac56e0&type=1&siteId=570a50fba2c146ca9efa552ed8300ec4&channelId=4611eeb3908c4772888589eaa439ace5&shareAppId=570a50fba2c146ca9efa552ed8300ec4">Shanghai Regional Competition</a>.</p>
      <p>[2023/05/11] Invited talk at <a href="https://iie.shu.edu.cn/info/1081/1321.htm">Shanghai University Institute and Entrepreneurship</a>.</p>
      <p>[2023/04/26] Invited talk at DJI RoboMaster 2023 <a href="https://iie.shu.edu.cn/info/1011/1241.htm">Shanghai Regional Competition</a>.</p>
      <p>[2023/03/30] Invited talk at <a href="https://ziqiangcol.shu.edu.cn/info/1048/1067.htm">Shanghai University Ziqiang College</a>.</p>
      <p>[2018/06/05] Invited talk at <a href="https://www.sohu.com/a/234144674_503549">Junior Edison TV Show</a>.</p>
      
    </section>
    <section>
      <h2>Selected Honors & Awards</h2>
      <p>[2023/08/20] First Prize of Sentry Robot at RoboMaster 2023 National Finals</p>
      <p>[2023/04/23] Third Place at RoboMaster 2023 Shanghai Regional Competition</p>
      <p>[2019/04/28] <a href="https://theorangealliance.org/teams/16107?season_key=1819">Admission</a> at 2019 FIRST World Championship, Houston</p>
      <p>[2018/06/01] Connect Award at 2017-2018 FIRST Tech Challenge China National Finals</p>
      <p>[2018/04/28] <a href="https://theorangealliance.org/teams/14263?season_key=1718">Admission</a> at 2018 FIRST World Championship, Detroit</p>
      <p>[2018/03/25] Inspire Award at 2017-2018 FIRST Tech Challenge Shanghai American School Qualifier</p>
    </section>



    <section>
      <h2>Misc</h2>
      <p> I love walking by the ocean, soaking in the sea breeze, and watching the sunsets. The beaches in La Jolla and Berkeley always leave me feeling refreshed and inspiring.</p>
      <div style="display: flex; flex-wrap:wrap; gap: 1em">
        <img
          src="./assets/sunset-1.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-2.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-3.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-4.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
      </div>

      <p> Bookshelf</p>
      <div class="book-gallery">
        <!-- <img src="./assets/books/cover-The World I See.jpg" alt="Book 1 Cover" onclick="window.open('', '_blank')" /> -->
        <img src="./assets/books/cover-The World I See.jpg" alt="Book 1 Cover" />
        <img src="./assets/books/cover-Outlier.jpg" alt="Book 2 Cover" />
        <img src="./assets/books/cover-The First Minute.jpg" alt="Book 3 Cover" />
        <img src="./assets/books/cover-Secret of Jobs.jpg" alt="Book 3 Cover" />
        <img src="./assets/books/cover-Make It Clear.jpg" alt="Book 3 Cover"/>
        <img src="./assets/books/cover-Gut.jpg" alt="Book 3 Cover" />
      </div>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
          <tr>
              <td style="padding:0px">
                  <br>
                  <br>
                  <div>
                      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=003262&w=180&t=n&d=6Fg6dc3zOBq_Wt0f7bRIsXMx3yMmcW-4XgUzlye8lck&co=ffffff&cmn=fdb515&cmo=2b8310'></script>
                  </div>
              </td>
          </tr>
      </tbody>
    </table>
      
      
    </section>
    <div style="height: 1em"></div>
    <section>
      <em style="color: #777">
        Website design borrows from
        <a href="https://jonbarron.info/">Jon Barron</a> and
        <a href="https://brentyi.github.io/">Brent Yi</a>.
        Forked from <a href="https://github.com/chungmin99/">Chung Min Kim</a>.
      </em>
    </section>
    <div style="height: 2em"></div>
  </body>
</html>