<!doctype html>
<html lang="en">
  <head>
    <title>Jonas Li: Forge The Future</title>

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <link href="./assets/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

  </head>
  <body>
    <div style="height: 4em"></div>
    <section
      style="
        display: flex;
        align-items: start;
        gap: 2em 1.5em;
        flex-wrap: wrap-reverse;
      "
    >
      <div style="flex-grow: 1; flex-basis: 20em">
        <h1>Yunzhe "Jonas" Li</h1>
        <!-- <p>where I am affiliated with the <a href="https://msc.berkeley.edu/">Mechanical Systems Control Lab</a> to work on humanoid robot manipulation</p> -->
        <p>
          I'm pursuing a master’s degree in the Department of EECS at <a href="https://eecs.berkeley.edu/">UC Berkeley</a>. I am graciously to be an alumnus of both <a href="https://www.robomaster.com/en-US">RoboMaster</a> and <a href="https://www.firstinspires.org/">FIRST</a>, and affiliated with <a href="https://www.dji.com/">DJI</a> as an event technical executive.
        </p>
        <p>
          Previously, I worked as a software engineer at <a href="https://www.momenta.cn/en/">Momenta</a>, contributing to the reversing feature for GM Cadillac Lyriq's autopilot system. My work included developing a clustering algorithm to detect ego vehicle stuck states across 30+ garages and creating a data processing tool to handle over 36,000 daily simulation data points.
        </p>
       <p>
          I earned a B.E. in Computer Science from <a href="https://en.shu.edu.cn/">Shanghai University</a>, where I led a 40-member team in the RoboMaster competition, developing 8 types of robots from scratch over 4 years and earning 3rd place in the Shanghai Regional Competition.
        </p>
        <p>
          My interest at the intersection of computer vision, AI, and robotics was ignited in high school, where I actively participated in the <a href="https://www.firstinspires.org/robotics/ftc">FIRST Tech Challenge</a> as a team lead. We achieved 2 FIRST World Championship admissions, 1 Inspire Award, and 4 Connect Awards over 3 years.
        </p>
      
        <p>
          <a href="mailto:liyunzhe.jonas@berkeley.edu">Email</a>
          &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/yunzhe-l-991638151/">LinkedIn</a>
          &nbsp;/&nbsp;
          <a href="https://www.youtube.com/@yunzheli5359">YouTube</a>
          &nbsp;/&nbsp;
          <a href="https://github.com/LIYunzhe1408">Github</a>
          &nbsp;/&nbsp;
          <a href="https://drive.google.com/file/d/1FaS0ec23HjVUJmM7Mn1wi09pJKsCdZF6/view?usp=sharing">Resume</a>
        </p>
      </div>
      <div style="display: block">
        <img
          src="./assets/self_portrait.jpg"
          alt="A photo of the author"
          style="
            width: 20em;
            height: auto;
            aspect-ratio: 1;
            object-fit: cover;
            border-radius: 5%;
          "
        />
      </div>
    </section>
    <section>
      <h2>Projects</h2>
      <p>⚙️ Updating In Progress ⚙️</p>
      <p> My projects focus on developing robots or systems that enhance human productivity by interacting with the physical world through images or text.</p>
      <!-- Paper list will be dynamically generated here -->
      <ul id="paper-list" style="margin-top: 1em" data-show-recent="false"></ul>
      <!-- [ <a href="#" onclick="toggleRecentPapers(event)">Show older projects</a> ] -->

      <!-- Generate paper list. -->
      <script>
        const papers = [
        {
            previewSrc: "./assets/Abstract.mp4",
            title:
              "Abstract: Your LeetCode Learning Assistant",
            authors: [
              "Jonas Li",
            ],
            conference: "Open-Source Contribution, 2025",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Abstract-LeetCode" },
              { text: "Video", url: "https://youtu.be/IpndzIX_nuw?si=3FaO41SYUJSazVCq" },
            ],
            summary:
              "ABSTRACT is an intelligent study and review tool designed specifically for mastering LeetCode questions. It helps users organize, summarize, and retain key problem-solving patterns efficiently. With structured note-taking, spaced repetition, and AI-powered insights, ABSTRACT makes coding interview preparation faster and more effective.",
            highlighted: False,
            recent: true,
            new: true,
          },  
        {
            previewSrc: "./assets/MealMate.mp4",
            title:
              "MealMate: From Craving to Cart",
            authors: [
              "Jonas Li",
              "Jasper Liu",
              "Mats Wiig Martinussen",
              "Emil Klovning",
              "Jason Ji",
              "Qinghe Zheng",
            ],
            conference: "LLM Agents MOOC Hackathon, Berkeley RDI, 2024",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/MealMate" },
              { text: "Video", url: "https://youtu.be/bAT-jZhDtCw?si=r7iHjxn-TGihWYJS" },
            ],
            summary:
              "MealMate is an intelligent meal planning assistant that simplifies food shopping by seamlessly integrating recipe generation, ingredient availability checks, and personalized substitutions. Powered by advanced multi-agent collaboration and large language models (LLMs), MealMate delivers tailored shopping lists based on user preferences and real-time store inventory.",
            highlighted: False,
            recent: true,
            new: False,
          },  
          {
            previewSrc: "./assets/RuneTrack.mp4",
            title:
              "RuneTrack: Real-Time Planar Non-Uniform Rotation Targeting",
            authors: [
              "Jonas Li",
              "Xinyu He",
              "Jun Chen",
              "Minfeng Tang",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2023",
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "The RuneTrack enhances real-time Energy Rune targeting in RoboMaster by replacing OpenCV-based detection with a YOLOv7-powered solution. A fine-tuned Rune detector, combined with a Least Squares-based tracker, retains 95% accuracy at 60 FPS on NVIDIA Jetson NX. The system computes roll, pitch, and yaw angles based on geometric information, ensuring precise gimbal control. The pipeline improves targeting stability and achieves 90% hit rate in dynamic competition scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/EngineerRobot.mp4",
            title:
              "OmniMiner: Adaptive Engineer Robot for High-Speed Object Collection and Redemption",
            authors: [
              "Chen Chen",
              "Daoming He",
              "Jonas Li",
              "Jingyi Li",
              "Kaicheng Li",
              "Hanqing Huang",
              "Qiming Pu"
            ],
            conference: "RoboMaster, 2023",
            links: [
              { text: "Video", url: "https://youtu.be/qeRvdmcNFAc?si=4-hF27Y6jXwbmtPL" },
            ],
            summary:
              "The Engineer Robot is a 5-axis robotic system on a mecanum-wheel chassis for efficient cubic object collection and redemption in RoboMaster. Using 11 motors to control XYZ, pitch, and roll, an air pump and suction cup system handles 20cm × 20cm × 20cm cubes. Automated sequences enable 0.8s intake time. A user-friendly UI ensures smooth operation, averaging 15s for a random position and orientation placement, outperforming 95% in intake speed and 70% in placement success rate.",
            highlighted: false,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/ArmorTrack.mp4",
            title:
              "ArmorTrack: A Full-Stack Vision Pipeline for Real-Time Armor Detection and Targeting",
            authors: [
              "Minfeng Tang",
              "Xinyu He",
              "Jun Chen",
              "Jonas Li",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2022",
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "This ArmorTrack is a real-time, adaptive vision pipeline for all RoboMaster vision systems, integrating camera calibration, image preprocessing, YOLOv7 detection, Kalman filter tracking, and fire control. Optimized for NVIDIA Jetson NX (60+ FPS), it features enemy motion mode detection, SolvePnP aiming correction, and robust exception handling for dynamic scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/HCE.mp4",
            title:
              "Hierarchical Concept-based Visual Explainer For Deep Learning Decisions",
            authors: [
              "Jonas Li",
              "Ziyi Yu",
              "Yue Liu",
            ],
            conference: "State Key Program of National Nature Science Foundation of China (No.61936001), Bachelor's Thesis, 2024",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Deep-Learning-Explainer/tree/v1.0-dataset" },
            ],
            summary:
              "HCE improves DNN explainability with a hierarchical concept-based method, extracting multilevel concepts via segmentation and clustering. It builds concept trees with an autoencoder and quantifies importance using Concept Tree Shapley values. A web explainer showcases HCE’s effectiveness, demonstrating 35% improved consistency in explanations across multiple image classification models.",
            highlighted: true,
            recent: true,
            new: false,
          },  
          {
            previewSrc: "./assets/NASICON.mp4",
            title:
              "Mining Property Relations of NASICON Solid Electrolyte",
            authors: [
              "Jonas Li",
              "Licheng Zhen",
              "Jiayao Zhang",
              "Xianyuan Ge",
              "Ziyi Yu",
              "Dahui Liu",
              "Yue Liu",
            ],
            conference: "General Program of National Natural Science Foundation of China (No.52073169), 2021",
            links: [
            ],
            summary:
              "This study enhances materials science text mining by improving Named Entity Recognition (NER) and relation extraction. A BERT-based pipeline extracts 106,896 entities and 260,475 entity-relation triples from 1,808 papers, boosting NER precision by 5%. A web platform enables material scientists to convert literature into property graphs for knowledge discovery.",
            highlighted: false,
            recent: true,
            new: false,
          }, 
        ];
        function generatePaperList() {
          const paperList = document.getElementById("paper-list");
          papers.forEach((paper) => {
            const li = document.createElement("li");
            if (paper.highlighted) {
              li.classList.add("paper-highlighted");
            }
            if (paper.recent) {
              li.classList.add("paper-recent");
            }
            li.classList.add("paper");
            li.innerHTML = `
              ${
                paper.previewSrc.endsWith(".mp4")
                  ? `<video class="paper-visual" src="${paper.previewSrc}" loop muted autoplay playsinline></video>`
                  : `<img class="paper-visual" src="${paper.previewSrc}" alt="${paper.title}">`
              }
              <div class="paper-textual">
                <h3>
                  ${paper.title}
                  ${paper.new ? '<img src="./assets/new_animated.gif" alt="Blinking icon that says \'new\'." />' : ""}
                </h3>
                <div style="height: 0.1em"></div>
                ${paper.authors
                  .map((author) => {
                    const names = author.split(" ");
                    const firstName = names.slice(0, -1).join("&nbsp;");
                    const lastName = names[names.length - 1];
                    const formattedName = `${firstName}&nbsp;${lastName}`;
                    return author.startsWith("Jonas Li")
                      ? `<strong>${formattedName}</strong>`
                      : formattedName;
                  })
                  .join(", ")}.
                <span style="font-weight: 450">${paper.conference}.</span>
                <div style="height: 0.375em"></div>
                ${paper.links
                  .map((link) => `<a href="${link.url}">${link.text}</a>`)
                  .join(" / ")}
                <div style="height: 0.375em"></div>
                <em style="opacity: 0.625">${paper.summary}</em>
              </div>
            `;
            paperList.appendChild(li);
          });
        }

        document.addEventListener("DOMContentLoaded", generatePaperList);

        function toggleRecentPapers(event) {
          event.preventDefault();
          const paperList = document.getElementById("paper-list");
          const showRecent = paperList.dataset.showRecent === "true";
          paperList.dataset.showRecent = (!showRecent).toString();
          event.target.textContent = showRecent
            ? "Show older papers"
            : "Hide older papers";
        }
      </script>
    </section>
    <section>
      <h2>Misc</h2>
      <p> I love walking by the ocean, soaking in the sea breeze, and watching the sunsets. The beaches in San Diego and Berkeley always leave me feeling refreshed and inspiring.</p>
      <div style="display: flex; flex-wrap:wrap; gap: 1em">
        <img
          src="./assets/sunset-1.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-2.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-3.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-4.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
      </div>      
    </section>
    <div style="height: 1em"></div>
    <section>
      <em style="color: #777">
        Website design borrows from
        <a href="https://jonbarron.info/">Jon Barron</a> and
        <a href="https://brentyi.github.io/">Brent Yi</a>.
        Forked from <a href="https://github.com/chungmin99/">Chung Min Kim</a>.
      </em>
    </section>
    <div style="height: 2em"></div>
  </body>
</html>
