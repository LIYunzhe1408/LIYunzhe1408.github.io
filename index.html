<!doctype html>
<html lang="en">
  <head>
    <title>Jonas Li: Forge The Future</title>

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <link href="./assets/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

  </head>
  <body>
    <div style="height: 4em"></div>
    <section
      style="
        display: flex;
        align-items: start;
        gap: 2em 1.5em;
        flex-wrap: wrap-reverse;
      "
    >
      <div style="flex-grow: 1; flex-basis: 20em">
        <h1>Yunzhe "Jonas" Li</h1>
        <!-- <p>where I am affiliated with the <a href="https://msc.berkeley.edu/">Mechanical Systems Control Lab</a> to work on humanoid robot manipulation</p> -->
        <p>
          I'm pursuing a master’s degree in the Department of EECS at <a href="https://eecs.berkeley.edu/">UC Berkeley</a>. I am graciously to be an alumnus of both <a href="https://www.robomaster.com/en-US">RoboMaster</a> and <a href="https://www.firstinspires.org/">FIRST</a>, and affiliated with <a href="https://www.dji.com/">DJI</a> as an event technical executive.
        </p>
        <p>
          Previously, I worked as a software engineer at <a href="https://www.momenta.cn/en/">Momenta</a>, contributing to the autopilot reversing feature for GM Cadillac LYRIQ. My work involved developing a stuck-state detection benchmark for reversing maneuvers, evaluating 800+ real parking test cases across 30+ garages, and optimizing SVM-based classification to reduce false positives by 15%.
        </p>
       <p>
          I earned a B.E. in Computer Science from <a href="https://en.shu.edu.cn/">Shanghai University</a>, where I led a 40-member team in the RoboMaster competition, developing 8 types of robots from scratch over 4 years and earning 3rd place in the Shanghai Regional Competition.
        </p>
        <p>
          My interest at the intersection of computer vision, AI, and robotics was ignited in high school, where I actively participated in the <a href="https://www.firstinspires.org/robotics/ftc">FIRST Tech Challenge</a> as a team lead. We achieved 2 FIRST World Championship admissions, 1 Inspire Award, and 4 Connect Awards over 3 years.
        </p>
        <p>Currently, I aim to explore <b>Vision-Language navigation</b> to address the limitations of vision-only models in understanding intent and natural human interaction</p>
      
        <p>
          <a href="mailto:liyunzhe.jonas@berkeley.edu">Email</a>
          &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/yunzhe-l-991638151/">LinkedIn</a>
          &nbsp;/&nbsp;
          <a href="https://www.youtube.com/@yunzheli5359">YouTube</a>
          &nbsp;/&nbsp;
          <a href="https://github.com/LIYunzhe1408">Github</a>
          &nbsp;/&nbsp;
          <a href="https://drive.google.com/file/d/17jS-61mbPj8e4mEsLsay-TkfmC1k8RP8/view?usp=sharing">Resume</a>
        </p>
      </div>
      <div style="display: block">
        <img
          src="./assets/self_portrait.jpg"
          alt="A photo of the author"
          style="
            width: 20em;
            height: auto;
            aspect-ratio: 1;
            object-fit: cover;
            border-radius: 5%;
          "
        />
      </div>
    </section>

    <section id="filters">
      <h2>Projects</h2>
      <p> "What motivates the development of AI must explicitly center on human benefit." -- Fei-Fei Li</p>
      <button onclick="filterPapers('all')">All</button>
      <button onclick="filterPapers('CVR')">Robotics & Computer Vision</button>
      <button onclick="filterPapers('LLM')">LLM & ML Application</button>
    </section>
    
    <section>
      <!-- Paper list will be dynamically generated here -->
      <ul id="paper-list" style="margin-top: 1em" data-show-recent="false"></ul>
      <!-- [ <a href="#" onclick="toggleRecentPapers(event)">Show older projects</a> ] -->

      <!-- Generate paper list. -->
      <script>
        const papers = [
        {
            previewSrc: "./assets/MNIST generate.gif",
            title:
              "Flow Matching For MNIST",
            authors: [
              "Jonas Li",
            ],
            conference: "Open-Source Contribution, 2025",
            
            category: "CVR",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Flow-Matching" },
              { text: "Website", url: "https://yunzhe-li.top/Flow-Matching/" },
            ],
            summary:
              "This project explores Flow Matching as a generative modeling technique by training a UNet-based denoiser on the MNIST dataset. The model progressively denoises images using flow matching instead of traditional diffusion models.",
            highlighted: false,
            recent: true,
            new: true,
          },  
        {
            previewSrc: "./assets/Abstract-concise.mp4",
            title:
              "Abstract: Your LeetCode Learning Assistant",
            authors: [
              "Jonas Li",
            ],
            conference: "Open-Source Contribution, 2025",
            category: "LLM",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Abstract-LeetCode" },
              { text: "Video", url: "https://youtu.be/IpndzIX_nuw?si=3FaO41SYUJSazVCq" },
            ],
            summary:
              "ABSTRACT is an intelligent study and review tool designed specifically for mastering LeetCode questions. It helps users organize, summarize, and retain key problem-solving patterns efficiently. With structured note-taking, spaced repetition, and AI-powered insights, ABSTRACT makes coding interview preparation faster and more effective.",
            highlighted: false,
            recent: true,
            new: false,
          },  
        {
            previewSrc: "./assets/MealMate.mp4",
            title:
              "MealMate: From Craving to Cart",
            authors: [
              "Jonas Li",
              "Jasper Liu",
              "Mats Wiig Martinussen",
              "Emil Klovning",
              "Jason Ji",
              "Qinghe Zheng",
            ],
            conference: "LLM Agents MOOC Hackathon, Berkeley RDI, 2024",
            category: "LLM",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/MealMate" },
              { text: "Video", url: "https://youtu.be/bAT-jZhDtCw?si=r7iHjxn-TGihWYJS" },
            ],
            summary:
              "MealMate is an intelligent meal planning assistant that simplifies food shopping by seamlessly integrating recipe generation, ingredient availability checks, and personalized substitutions. Powered by advanced multi-agent collaboration and large language models (LLMs), MealMate delivers tailored shopping lists based on user preferences and real-time store inventory.",
            highlighted: false,
            recent: true,
            new: false,
          },  
          {
            previewSrc: "./assets/RuneTrack.mp4",
            title:
              "RuneTrack: Real-Time Planar Non-Uniform Rotation Targeting",
            authors: [
              "Jonas Li",
              "Xinyu He",
              "Jun Chen",
              "Minfeng Tang",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2023",
            category: "CVR",
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "The RuneTrack enhances real-time Energy Rune targeting in RoboMaster by replacing OpenCV-based detection with a YOLOv7-powered solution. A fine-tuned Rune detector, combined with a Least Squares-based tracker, retains 95% accuracy at 60 FPS on NVIDIA Jetson NX. The system computes roll, pitch, and yaw angles based on geometric information, ensuring precise gimbal control. The pipeline improves targeting stability and achieves 90% hit rate in dynamic competition scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/EngineerRobot.mp4",
            title:
              "OmniMiner: Adaptive Engineer Robot for High-Speed Object Collection and Redemption",
            authors: [
              "Chen Chen",
              "Daoming He",
              "Jonas Li",
              "Jingyi Li",
              "Kaicheng Li",
              "Hanqing Huang",
              "Qiming Pu"
            ],
            conference: "RoboMaster, 2023",
            category: "CVR",
            links: [
              { text: "Video", url: "https://youtu.be/qeRvdmcNFAc?si=4-hF27Y6jXwbmtPL" },
            ],
            summary:
              "The Engineer Robot is a 5-axis robotic system on a mecanum-wheel chassis for efficient cubic object collection and redemption in RoboMaster. Using 11 motors to control XYZ, pitch, and roll, an air pump and suction cup system handles 20cm × 20cm × 20cm cubes. Automated sequences enable 0.8s intake time. A user-friendly UI ensures smooth operation, averaging 15s for a random position and orientation placement, outperforming 95% in intake speed and 70% in placement success rate.",
            highlighted: false,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/ArmorTrack.mp4",
            title:
              "ArmorTrack: A Full-Stack Vision Pipeline for Real-Time Armor Detection and Targeting",
            authors: [
              "Minfeng Tang",
              "Xinyu He",
              "Jun Chen",
              "Jonas Li",
              "Zhengyu Li",
              "Jun Jiang",
            ],
            conference: "RoboMaster, 2022",
            category: "CVR",
            links: [
              { text: "Git repo", url: "https://github.com/SRM-Vision/SRM-Vision-2022" },
              { text: "Video", url: "https://www.youtube.com/watch?v=4uyBBJRXUTg" },
            ],
            summary:
              "This ArmorTrack is a real-time, adaptive vision pipeline for all RoboMaster vision systems, integrating camera calibration, image preprocessing, YOLOv7 detection, Kalman filter tracking, and fire control. Optimized for NVIDIA Jetson NX (60+ FPS), it features enemy motion mode detection, SolvePnP aiming correction, and robust exception handling for dynamic scenarios.",
            highlighted: true,
            recent: true,
            new: false,
          },
          {
            previewSrc: "./assets/HCE.mp4",
            title:
              "Hierarchical Concept-based Visual Explainer For Deep Learning Decisions",
            authors: [
              "Jonas Li",
              "Ziyi Yu",
              "Yue Liu",
            ],
            conference: "State Key Program of National Nature Science Foundation of China (No.61936001), Bachelor's Thesis, 2024",
            category: "LLM",
            links: [
              { text: "Git repo", url: "https://github.com/LIYunzhe1408/Deep-Learning-Explainer/tree/v1.0-dataset" },
              { text: "Video", url:"https://youtu.be/9Tyr53aP-r8?si=XDSroY1QU7EiMskY"}
            ],
            summary:
              "HCE improves DNN explainability with a hierarchical concept-based method, extracting multilevel concepts via segmentation and clustering. It builds concept trees with an autoencoder and quantifies importance using Concept Tree Shapley values. A web explainer showcases HCE’s effectiveness, demonstrating 35% improved consistency in explanations across multiple image classification models.",
            highlighted: true,
            recent: true,
            new: false,
          },  
          {
            previewSrc: "./assets/NASICON.mp4",
            title:
              "Mining Property Relations of NASICON Solid Electrolyte",
            authors: [
              "Jonas Li",
              "Licheng Zhen",
              "Jiayao Zhang",
              "Xianyuan Ge",
              "Ziyi Yu",
              "Dahui Liu",
              "Yue Liu",
            ],
            conference: "General Program of National Natural Science Foundation of China (No.52073169), 2021",
            category: "LLM",
            links: [
            ],
            summary:
              "This study enhances materials science text mining by improving Named Entity Recognition (NER) and relation extraction. A BERT-based pipeline extracts 106,896 entities and 260,475 entity-relation triples from 1,808 papers, boosting NER precision by 5%. A web platform enables material scientists to convert literature into property graphs for knowledge discovery.",
            highlighted: false,
            recent: true,
            new: false,
          }, 
        ];
        function generatePaperList(filteredCategory = 'all') {
          const paperList = document.getElementById("paper-list");
  paperList.innerHTML = ""; // clear existing list

  const filteredPapers = papers.filter(paper => {
    return filteredCategory === 'all' || paper.category === filteredCategory;
  });

  filteredPapers.forEach((paper) => {
    const li = document.createElement("li");
    if (paper.highlighted) {
      li.classList.add("paper-highlighted");
    }
    if (paper.recent) {
      li.classList.add("paper-recent");
    }
    li.classList.add("paper");
    li.innerHTML = `
      ${
        paper.previewSrc.endsWith(".mp4")
          ? `<video class="paper-visual" src="${paper.previewSrc}" loop muted autoplay playsinline></video>`
          : `<img class="paper-visual" src="${paper.previewSrc}" alt="${paper.title}">`
      }
      <div class="paper-textual">
        <h3>
          ${paper.title}
          ${paper.new ? '<img src="./assets/new_animated.gif" alt="New icon"/>' : ""}
        </h3>
        <div style="height: 0.1em"></div>
        ${paper.authors
          .map((author) => {
            const names = author.split(" ");
            const firstName = names.slice(0, -1).join("&nbsp;");
            const lastName = names[names.length - 1];
            const formattedName = `${firstName}&nbsp;${lastName}`;
            return author.startsWith("Jonas Li")
              ? `<strong>${formattedName}</strong>`
              : formattedName;
          })
          .join(", ")}.
        <span style="font-weight: 450">${paper.conference}.</span>
        <div style="height: 0.375em"></div>
        ${paper.links
          .map((link) => `<a href="${link.url}">${link.text}</a>`)
          .join(" / ")}
        <div style="height: 0.375em"></div>
        <em style="opacity: 0.625">${paper.summary}</em>
      </div>
    `;
    paperList.appendChild(li);
  });
        }

        // Filter function triggered by the buttons
        function filterPapers(category) {
          generatePaperList(category);
        }

        document.addEventListener("DOMContentLoaded", () => generatePaperList());

        function toggleRecentPapers(event) {
          event.preventDefault();
          const paperList = document.getElementById("paper-list");
          const showRecent = paperList.dataset.showRecent === "true";
          paperList.dataset.showRecent = (!showRecent).toString();
          event.target.textContent = showRecent
            ? "Show older papers"
            : "Hide older papers";
        }
      </script>
    </section>
    <section>
      <h2>Service</h2>
      <h3>Community Services</h3>
      <p>[2024/04/02] Head Referee at DJI RoboMaster 2024 Shanghai Regional Competition</p>
      <p>[2024/01/27] Lead Robot Inspector at 2023-2024 FIRST Tech Challenge Shanghai Qualifier</p>
      <p>[2021/05/14] Referee at 2020-2021 FIRST Tech Challenge Hangzhou Qualifier</p>
      <p>[2020/09/20] Robot Inspector at 2019-2020 FIRST Tech Challenge Shanghai Offseason Event</p>

      <h3>Media Coverage</h3>
      <p>[2024/04/02] Invited talk at DJI RoboMaster 2024 <a href="https://www.media.xinhuamm.net/html/rft/vod.html?programId=df2a93cb4ef04a20adebe284fe17055a&vodId=cab0a811860000014e301c601fac56e0&type=1&siteId=570a50fba2c146ca9efa552ed8300ec4&channelId=4611eeb3908c4772888589eaa439ace5&shareAppId=570a50fba2c146ca9efa552ed8300ec4">Shanghai Regional Competition</a>.</p>
      <p>[2023/05/11] Invited talk at <a href="https://iie.shu.edu.cn/info/1081/1321.htm">Shanghai University Institute and Entrepreneurship</a>.</p>
      <p>[2023/04/26] Invited talk at DJI RoboMaster 2023 <a href="https://iie.shu.edu.cn/info/1011/1241.htm">Shanghai Regional Competition</a>.</p>
      <p>[2023/03/30] Invited talk at <a href="https://ziqiangcol.shu.edu.cn/info/1048/1067.htm">Shanghai University Ziqiang College</a>.</p>
      <p>[2018/06/05] Invited talk at <a href="https://www.sohu.com/a/234144674_503549">Junior Edison TV Show</a>.</p>
      
    </section>
    <section>
      <h2>Selected Honors & Awards</h2>
      <p>[2023/08/20] First Prize of Sentry Robot at RoboMaster 2023 National Finals</p>
      <p>[2023/04/23] Third Place at RoboMaster 2023 Shanghai Regional Competition</p>
      <p>[2019/04/28] <a href="https://theorangealliance.org/teams/16107?season_key=1819">Admission</a> at 2019 FIRST World Championship, Houston</p>
      <p>[2018/06/01] Connect Award at 2017-2018 FIRST Tech Challenge China National Finals</p>
      <p>[2018/04/28] <a href="https://theorangealliance.org/teams/14263?season_key=1718">Admission</a> at 2018 FIRST World Championship, Detroit</p>
      <p>[2018/03/25] Inspire Award at 2017-2018 FIRST Tech Challenge Shanghai American School Qualifier</p>
    </section>
    <section>
      <h2>My Teams and Robots</h2>
      <div class="gallery">
        <img src="./assets/robots/FTC2018.JPG" style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/FTCWorld.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/FTCChina.JPG"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/FTC2019.JPG"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/FTC2020.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>

        <img src="./assets/robots/SRM2022.png"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM2023.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM2023-2.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM2024.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/RM2024.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>

        <img src="./assets/robots/R-2017.JPG"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/R-FTC2018.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/R-FTC2019.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/R-FTC2020.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/R-FTC2021.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>

        <img src="./assets/robots/SRM-Engineer.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM-HERO.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM-Sentry.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/SRM-Standard.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
        <img src="./assets/robots/ALL.jpg"style="width: calc(20% - 1em);height: auto;aspect-ratio: 1.5;object-fit: cover;border-radius: 5%;"/>
      </div>
    </section>
    <section>
      <h2>Misc</h2>
      <p> I love walking by the ocean, soaking in the sea breeze, and watching the sunsets. The beaches in La Jolla and Berkeley always leave me feeling refreshed and inspiring.</p>
      <div style="display: flex; flex-wrap:wrap; gap: 1em">
        <img
          src="./assets/sunset-1.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-2.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-3.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
        <img
          src="./assets/sunset-4.jpg"
          alt="Photos of sunsets at Berkeley and San Diego, taken by the author"
          style="
            width: 45%;
            height: auto;
            aspect-ratio: 1.5;
            object-fit: cover;
            border-radius: 5%;
          "
        />
      </div>      
    </section>
    <div style="height: 1em"></div>
    <section>
      <em style="color: #777">
        Website design borrows from
        <a href="https://jonbarron.info/">Jon Barron</a> and
        <a href="https://brentyi.github.io/">Brent Yi</a>.
        Forked from <a href="https://github.com/chungmin99/">Chung Min Kim</a>.
      </em>
    </section>
    <div style="height: 2em"></div>
  </body>
</html>
